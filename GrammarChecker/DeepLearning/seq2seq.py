# -*- coding: utf-8 -*-
"""Seq2Seq_LSTM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dRvlch14_Dk45fwISdwcOWCC9MjFmqDh
"""



import pandas as pd
import numpy as np
import random
import matplotlib.pyplot as plt
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, LSTM, Dense, Embedding
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split
from nltk.translate.bleu_score import sentence_bleu
from tensorflow.keras.optimizers import Adam

# Load dataset
file_path = r'D:\7th_Semester_FoE_UoJ\EC9640_Artificial Intelligence\Project\GrammarChecker\DeepLearning\DataSet\TamilDatasetGrammar.xlsx'
df = pd.read_excel(file_path)

# Clean dataset
df = df.dropna(subset=['Original Sentence', 'Corrected Sentence'])
df['Original Sentence'] = df['Original Sentence'].astype(str).str.strip()
df['Corrected Sentence'] = df['Corrected Sentence'].astype(str).str.strip()

# Add start and end tokens for decoding
input_sentences = df['Original Sentence'].values
target_sentences = ['<start> ' + sentence + ' <end>' for sentence in df['Corrected Sentence'].values]

# Tokenize input and target sentences
input_tokenizer = Tokenizer(filters='', oov_token='<unk>')
input_tokenizer.fit_on_texts(input_sentences)
input_sequences = input_tokenizer.texts_to_sequences(input_sentences)

output_tokenizer = Tokenizer(filters='', oov_token='<unk>')
output_tokenizer.fit_on_texts(target_sentences)
target_sequences = output_tokenizer.texts_to_sequences(target_sentences)

# Vocabulary sizes
input_vocab_size = len(input_tokenizer.word_index) + 1
output_vocab_size = len(output_tokenizer.word_index) + 1

# Maximum sequence lengths
max_input_length = max(len(seq) for seq in input_sequences)
max_target_length = max(len(seq) for seq in target_sequences)

# Pad sequences
encoder_input_data = pad_sequences(input_sequences, maxlen=max_input_length, padding='post')
decoder_input_data = pad_sequences(target_sequences, maxlen=max_target_length, padding='post')

# Decoder output data
decoder_output_data = np.zeros((len(target_sequences), max_target_length, output_vocab_size), dtype='float32')
for i, seq in enumerate(target_sequences):
    for t, word_id in enumerate(seq):
        if t > 0:  # Skip the first token
            decoder_output_data[i, t - 1, word_id] = 1.0

# Train-test split
encoder_input_train, encoder_input_val, decoder_input_train, decoder_input_val, decoder_output_train, decoder_output_val = train_test_split(
    encoder_input_data, decoder_input_data, decoder_output_data, test_size=0.2, random_state=42)

# Model parameters
embedding_dim = 512  # Increased embedding dimension
hidden_units = 1024  # Increased hidden units
learning_rate = 0.001  # Adjusted learning rate

# Encoder
encoder_inputs = Input(shape=(max_input_length,))
encoder_embedding = Embedding(input_vocab_size, embedding_dim, mask_zero=True)(encoder_inputs)
encoder_outputs, state_h, state_c = LSTM(hidden_units, return_state=True)(encoder_embedding)
encoder_states = [state_h, state_c]

# Decoder
decoder_inputs = Input(shape=(max_target_length,))
decoder_embedding = Embedding(output_vocab_size, embedding_dim, mask_zero=True)(decoder_inputs)
decoder_lstm = LSTM(hidden_units, return_sequences=True, return_state=True)
decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)
decoder_dense = Dense(output_vocab_size, activation='softmax')
decoder_outputs = decoder_dense(decoder_outputs)

# Define the model
model = Model([encoder_inputs, decoder_inputs], decoder_outputs)
optimizer = Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)  # Optimized Adam
model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])

# Train the model
batch_size = 64
epochs = 100  # Increased epochs for better learning

history = model.fit(
    [encoder_input_train, decoder_input_train],
    decoder_output_train,
    batch_size=batch_size,
    epochs=epochs,
    validation_data=([encoder_input_val, decoder_input_val], decoder_output_val)
)

import os
import pickle
from tensorflow.keras.models import load_model

# Function to save models in h5 format and tokenizers in pickle format
def save_models_and_tokenizers(encoder_model, decoder_model, input_tokenizer, output_tokenizer, base_path='models/'):
    os.makedirs(base_path, exist_ok=True)  # Create the directory if it does not exist

    # Save models
    encoder_model_path = os.path.join(base_path, 'encoder_model.h5')
    decoder_model_path = os.path.join(base_path, 'decoder_model.h5')
    encoder_model.save(encoder_model_path)
    decoder_model.save(decoder_model_path)

    # Save tokenizers
    input_tokenizer_path = os.path.join(base_path, 'input_tokenizer.pkl')
    output_tokenizer_path = os.path.join(base_path, 'output_tokenizer.pkl')
    with open(input_tokenizer_path, 'wb') as handle:
        pickle.dump(input_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)
    with open(output_tokenizer_path, 'wb') as handle:
        pickle.dump(output_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)

    print("Models and tokenizers have been saved successfully.")

# Call the function with the models and tokenizers
save_models_and_tokenizers(encoder_model, decoder_model, input_tokenizer, output_tokenizer)

# Plot accuracy and loss
plt.figure(figsize=(12, 6))
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

plt.figure(figsize=(12, 6))
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

# Encoder model for inference
encoder_model = Model(encoder_inputs, encoder_states)

# Decoder model for inference
decoder_state_input_h = Input(shape=(hidden_units,))
decoder_state_input_c = Input(shape=(hidden_units,))
decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]
decoder_embedding_inf = Embedding(output_vocab_size, embedding_dim, mask_zero=True)(decoder_inputs)
decoder_outputs, state_h, state_c = decoder_lstm(decoder_embedding_inf, initial_state=decoder_states_inputs)
decoder_states = [state_h, state_c]
decoder_outputs = decoder_dense(decoder_outputs)
decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)

# Decode function
def decode_sequence(input_seq):
    states_value = encoder_model.predict(input_seq)
    target_seq = np.zeros((1, 1))
    target_seq[0, 0] = output_tokenizer.word_index['<start>']
    stop_condition = False
    decoded_sentence = ''
    while not stop_condition:
        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)
        sampled_token_index = np.argmax(output_tokens[0, -1, :])
        sampled_word = output_tokenizer.index_word.get(sampled_token_index, '<unk>')
        if sampled_word == '<end>' or len(decoded_sentence.split()) > max_target_length:
            stop_condition = True
        else:
            decoded_sentence += ' ' + sampled_word
        target_seq[0, 0] = sampled_token_index
        states_value = [h, c]
    return decoded_sentence.strip()

import os
import pickle
from tensorflow.keras.models import load_model

# Function to save models in h5 format and tokenizers in pickle format
def save_models_and_tokenizers(encoder_model, decoder_model, input_tokenizer, output_tokenizer, base_path='models/'):
    os.makedirs(base_path, exist_ok=True)  # Create the directory if it does not exist

    # Save models
    encoder_model_path = os.path.join(base_path, 'encoder_model.h5')
    decoder_model_path = os.path.join(base_path, 'decoder_model.h5')
    encoder_model.save(encoder_model_path)
    decoder_model.save(decoder_model_path)

    # Save tokenizers
    input_tokenizer_path = os.path.join(base_path, 'input_tokenizer.pkl')
    output_tokenizer_path = os.path.join(base_path, 'output_tokenizer.pkl')
    with open(input_tokenizer_path, 'wb') as handle:
        pickle.dump(input_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)
    with open(output_tokenizer_path, 'wb') as handle:
        pickle.dump(output_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)

    print("Models and tokenizers have been saved successfully.")

# Call the function with the models and tokenizers
save_models_and_tokenizers(encoder_model, decoder_model, input_tokenizer, output_tokenizer)

# Random Sentence Testing and Accuracy
random_indices = random.sample(range(len(input_sentences)), 10)
total_bleu = 0
correct_predictions = 0

print("\n--- Random Sentence Testing ---\n")
for i in random_indices:
    original_sentence = input_sentences[i]
    correct_sentence = df['Corrected Sentence'].iloc[i]
    input_seq = pad_sequences(input_tokenizer.texts_to_sequences([original_sentence]), maxlen=max_input_length, padding='post')
    predicted_sentence = decode_sequence(input_seq)

    bleu = sentence_bleu([correct_sentence.split()], predicted_sentence.split(), weights=(0.5, 0.5))
    total_bleu += bleu
    is_correct = predicted_sentence.strip() == correct_sentence.strip()
    correct_predictions += int(is_correct)

    print(f"Original: {original_sentence}")
    print(f"Predicted: {predicted_sentence}")
    print(f"Corrected: {correct_sentence}")
    print(f"Error Type: {'Incorrect Prediction' if not is_correct else 'Correct Prediction'}")
    print(f"BLEU Score: {bleu:.4f}\n")

    # Calculate and display accuracy
accuracy = correct_predictions / len(random_indices)
average_bleu = total_bleu / len(random_indices)

print("\n--- Model Evaluation ---")
print(f"Accuracy: {accuracy * 100:.2f}%")
print(f"Average BLEU Score: {average_bleu:.4f}")

from google.colab import drive
drive.mount('/content/drive')

import re

# Function to input a Tamil paragraph, split into sentences, check each for grammar errors, and print accuracy
def input_and_check_paragraph():
    input_paragraph = input("Enter a Tamil paragraph for grammar check: ")

    # Split the paragraph into sentences
    sentences = re.split(r'[.,!?]', input_paragraph)  # Split by punctuation marks like .!?
    sentences = [sentence.strip() for sentence in sentences if sentence]  # Clean up empty sentences

    total_sentences = len(sentences)
    correct_sentences = 0
    total_bleu = 0

    print("\n--- Checking Each Sentence ---\n")
    for sentence in sentences:
        input_seq = pad_sequences(input_tokenizer.texts_to_sequences([sentence]), maxlen=max_input_length, padding='post')
        predicted_sentence = decode_sequence(input_seq)

        # In this case, we need the actual corrected sentence, which is not available directly from input, so we compare with a 'blank' or previous knowledge
        # For now, we will print and manually check, or create a simple 'correct' assumption.

        print(f"Original Sentence: {sentence}")
        print(f"Predicted Sentence: {predicted_sentence}")

        bleu = sentence_bleu([sentence.split()], predicted_sentence.split(), weights=(0.5, 0.5))
        total_bleu += bleu
        if predicted_sentence.strip() == sentence.strip():  # Compare with the input directly (manual check)
            correct_sentences += 1

        print(f"BLEU Score for the sentence: {bleu:.4f}")
        print(f"Correct Prediction: {'Yes' if predicted_sentence.strip() == sentence.strip() else 'No'}")
        print("---")

    accuracy = correct_sentences / total_sentences
    average_bleu = total_bleu / total_sentences

    print(f"\n--- Final Results ---")
    print(f"Accuracy of Grammar Check: {accuracy * 100:.2f}%")
    print(f"Average BLEU Score: {average_bleu:.4f}")

# Example of using the function
input_and_check_paragraph()

# Random Sentence Testing and Accuracy
random_indices = random.sample(range(len(input_sentences)), 10)
total_bleu = 0
correct_predictions = 0
bleu_threshold = 0.5  # You can set the threshold for considering a prediction as correct

print("\n--- Random Sentence Testing ---\n")
for i in random_indices:
    original_sentence = input_sentences[i]
    correct_sentence = df['Corrected Sentence'].iloc[i]
    input_seq = pad_sequences(input_tokenizer.texts_to_sequences([original_sentence]), maxlen=max_input_length, padding='post')
    predicted_sentence = decode_sequence(input_seq)

    bleu = sentence_bleu([correct_sentence.split()], predicted_sentence.split(), weights=(0.5, 0.5))
    total_bleu += bleu

    # Adjusted condition for correctness based on BLEU score threshold
    is_correct = bleu >= bleu_threshold  # Consider prediction correct if BLEU score is above threshold
    correct_predictions += int(is_correct)

    print(f"Original: {original_sentence}")
    print(f"Predicted: {predicted_sentence}")
    print(f"Corrected: {correct_sentence}")
    print(f"Error Type: {'Incorrect Prediction' if not is_correct else 'Correct Prediction'}")
    print(f"BLEU Score: {bleu:.4f}\n")

# Calculate and display accuracy
accuracy = correct_predictions / len(random_indices)
average_bleu = total_bleu / len(random_indices)

print("\n--- Model Evaluation ---")
print(f"Accuracy: {accuracy * 100:.2f}%")
print(f"Average BLEU Score: {average_bleu:.4f}")

